{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<center><b>AIM404 : Contextual Bandits with Amazon SageMaker RL - Bandits model deployment with the end-to-end loop</b></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cells explained the individual steps in the training workflow. To train a model to convergence, we will continually train the model based on data collected with client application interactions. We demonstrate the continual training loop in a single cell below.\n",
    "\n",
    "We include the evaluation step at each step before deployment to compare the model just trained (`last_trained_model_id`) against the model that is currently hosted (`last_hosted_model_id`). If you want the loops to finish faster, you can set `do_evaluation=False` in the cell below.\n",
    "\n",
    "Details of each joining and training job can be tracked in `join_db` and `model_db` respectively. `model_db` also stores the evaluation scores. When you have multiple experiments, you can check their status in `experiment_db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import sagemaker\n",
    "import pprint\n",
    "import pandas as pd\n",
    "sys.path.append('common')\n",
    "sys.path.append('common/sagemaker_rl')\n",
    "from misc import get_execution_role\n",
    "from markdown_helper import *\n",
    "from IPython.display import Markdown\n",
    "from IPython.core.display import Image, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('images/AIM404-workflow.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"\">\n",
    "<b>IMPORTANT :</b> In order to speed the training process in the loop, we will run it in local mode true and soft deployment true. You need to edit the file <b>config-loop.yaml</b>\n",
    "<ul>\n",
    "                                            <li>local_mode: <b>true</b></li>\n",
    "                                            <li>soft_deployment: <b>true</b></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('sim_app')\n",
    "from statlog_sim_app import StatlogSimApp\n",
    "from sim_app_utils import *\n",
    "from orchestrator.workflow.manager.experiment_manager import ExperimentManager\n",
    "with open('config-loop.yaml', 'r') as yaml_file:\n",
    "    config = yaml.load(yaml_file)\n",
    "\n",
    "do_evaluation = True\n",
    "\n",
    "experiment_name = \"AIM404-Loop\" #YOUR EXPERIMENT NAME HERE \n",
    "bandits_experiment = ExperimentManager(config, experiment_id=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "total_loops = 10 # Increase for higher accuracy\n",
    "batch_size = 100 # for the warm start\n",
    "rewards_list = []\n",
    "\n",
    "local_mode = bandits_experiment.local_mode\n",
    "\n",
    "# upload to s3\n",
    "from sim_app_utils import *\n",
    "warm_start_data_buffer = prepare_statlog_warm_start_data(data_file='sim_app/shuttle.trn', batch_size=batch_size)\n",
    "bandits_experiment.ingest_joined_data(warm_start_data_buffer,ratio=0.8)\n",
    "\n",
    "# first model training\n",
    "bandits_experiment.initialize_first_model(input_data_s3_prefix=bandits_experiment.last_joined_job_train_data) \n",
    "# first model deployment\n",
    "bandits_experiment.deploy_model(model_id=bandits_experiment.last_trained_model_id) \n",
    "# setup predictor for inference\n",
    "predictor = bandits_experiment.predictor\n",
    "sim_app = StatlogSimApp(predictor=predictor)\n",
    "\n",
    "assert sim_app.num_actions == bandits_experiment.config[\"algor\"][\"algorithms_parameters\"][\"num_arms\"]\n",
    "\n",
    "batch_size = 500 # for the loops\n",
    "\n",
    "for loop_no in range(total_loops):\n",
    "    print(f\"\"\"\n",
    "    #################\n",
    "    #################\n",
    "         Loop {loop_no+1}\n",
    "    #################\n",
    "    #################\n",
    "    \"\"\")\n",
    "    \n",
    "    # Generate experiences and log them\n",
    "    for i in range(batch_size):\n",
    "        user_id, user_context = sim_app.choose_random_user()\n",
    "        action, event_id, model_id, action_prob, sample_prob = predictor.get_action(obs=user_context.tolist())\n",
    "        reward = sim_app.get_reward(user_id, action, event_id, model_id, action_prob, sample_prob, local_mode)\n",
    "        rewards_list.append(reward)\n",
    "    \n",
    "    \n",
    "    # publish rewards sum for this batch to CloudWatch for monitoring \n",
    "    bandits_experiment.cw_logger.publish_rewards_for_simulation(\n",
    "        bandits_experiment.experiment_id,\n",
    "        sum(rewards_list[-batch_size:])/batch_size\n",
    "    )\n",
    "    \n",
    "    # Local/Athena join\n",
    "    if local_mode:\n",
    "        bandits_experiment.ingest_joined_data(sim_app.joined_data_buffer,ratio=0.85)\n",
    "    else:\n",
    "        print(\"Waiting for firehose to flush data to s3...\")\n",
    "        time.sleep(60) \n",
    "        rewards_s3_prefix = bandits_experiment.ingest_rewards(sim_app.rewards_buffer)\n",
    "        bandits_experiment.join(rewards_s3_prefix, ratio=0.85)\n",
    "    \n",
    "    # Train \n",
    "    bandits_experiment.train_next_model(\n",
    "        input_data_s3_prefix=bandits_experiment.last_joined_job_train_data)\n",
    "    \n",
    "    if do_evaluation:\n",
    "    # Evaluate\n",
    "        bandits_experiment.evaluate_model(\n",
    "            input_data_s3_prefix=bandits_experiment.last_joined_job_eval_data,\n",
    "            evaluate_model_id=bandits_experiment.last_trained_model_id)\n",
    "        eval_score_last_trained_model = bandits_experiment.get_eval_score(\n",
    "            evaluate_model_id=bandits_experiment.last_trained_model_id,\n",
    "            eval_data_path=bandits_experiment.last_joined_job_eval_data)\n",
    "\n",
    "        bandits_experiment.evaluate_model(\n",
    "            input_data_s3_prefix=bandits_experiment.last_joined_job_eval_data,\n",
    "            evaluate_model_id=bandits_experiment.last_hosted_model_id)\n",
    "\n",
    "        eval_score_last_hosted_model = bandits_experiment.get_eval_score(\n",
    "            evaluate_model_id=bandits_experiment.last_hosted_model_id, \n",
    "            eval_data_path=bandits_experiment.last_joined_job_eval_data)\n",
    "    \n",
    "        # Deploy\n",
    "        if eval_score_last_trained_model <= eval_score_last_hosted_model:\n",
    "            bandits_experiment.deploy_model(model_id=bandits_experiment.last_trained_model_id)\n",
    "            print ('Eval score in this context is actually the cost. It is calculated as 1 - mean reward')\n",
    "            print ('Meaning, we should deploy the new model only if its evaluation score is smaller, otherwise not')\n",
    "            print ('Eval score of the new model:',eval_score_last_trained_model)\n",
    "            print ('Eval score of the old model:',eval_score_last_hosted_model)\n",
    "            print ('We deploy the model in loop {}'.format({loop_no+1}))\n",
    "        else:\n",
    "            print ('Eval score in this context is actually the cost. It is calculated as 1 - mean reward')\n",
    "            print ('Meaning, we should deploy the new model only if its evaluation score is smaller, otherwise not')\n",
    "            print ('Eval score of the new model:',eval_score_last_trained_model)\n",
    "            print ('Eval score of the old model:',eval_score_last_hosted_model)\n",
    "            print('Not deploying model in loop {}'.format({loop_no+1}))\n",
    "    else:\n",
    "        bandits_experiment.deploy_model(model_id=bandits_experiment.last_trained_model_id)\n",
    "    \n",
    "    sim_app.clear_buffer()\n",
    "\n",
    "print(f\"Total time taken to complete {total_loops} loops: {time.time() - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualization'></a>\n",
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the model performance along the training loop by plotting the rolling mean reward across client interactions. Here rolling mean reward is calculated on the last `rolling_window` number of data instances, where each data instance corresponds to a single client interaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Note: The plot below cannot be generated if the notebook has been restarted after the execution of the cell above. \n",
    "</div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "def get_mean_reward(reward_lst, batch_size=batch_size):\n",
    "    mean_rew=list()\n",
    "    for r in range(len(reward_lst)):\n",
    "        mean_rew.append(sum(reward_lst[:r+1]) * 1.0 / ((r+1)*batch_size))\n",
    "    return mean_rew\n",
    "\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "lwd = 5\n",
    "cmap = plt.get_cmap('tab20')\n",
    "colors=plt.cm.tab20(np.linspace(0, 1, 20))\n",
    "\n",
    "rolling_window = 100\n",
    "rewards_df = pd.DataFrame(rewards_list, columns=['bandit']).rolling(rolling_window).mean()\n",
    "rewards_df['oracle'] = sum(sim_app.opt_rewards) / len(sim_app.opt_rewards)\n",
    "\n",
    "rewards_df.plot(y=['bandit','oracle'],linewidth=lwd)\n",
    "plt.legend(loc=4, prop={'size': 20})\n",
    "plt.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.xlabel('Data instances (models were updated every %s data instances)' % batch_size, size=20)\n",
    "plt.ylabel('Rolling Mean Reward', size=30)\n",
    "plt.ylim(0,1.2)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get mean rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_df.bandit.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't manage to finish the loop training, below is an example with 20 iterations of 500 for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('images/AIM404-reward-graph.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    Here we visualize the reward after a loop of 20 iterations with 500 batches for each iteration</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean-up'></a>\n",
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "If you want to start again the loop, you need to clean your experiment\n",
    "</div/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits_experiment.clean_resource(experiment_id=bandits_experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits_experiment.clean_table_records(experiment_id=bandits_experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean-up'></a>\n",
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start to optimize the results above by tweaking configurations of your vowpal wabbit algorithms. Hints look for the **hyperparameters!**\n",
    "* exploration_policy\n",
    "* num_policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
